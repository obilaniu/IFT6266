{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Using the backpropagation principle, find the derivatives of the loss function with respect to parameters $W_{i,j}$, $b_{j}$, $V_{jk}$, $c_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP model we use is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}' &= \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\\\\n",
    "\\mathbf{h}  &= \\sigma(\\mathbf{h}') \\\\\n",
    "\\mathbf{y}' &= \\mathbf{V}\\mathbf{h} + \\mathbf{c} \\\\\n",
    "\\mathbf{y}  &= \\mathrm{softmax}\\left( \\mathbf{y}' \\right) \\\\\n",
    "\\mathcal{L} &= \\sum\\limits_{k} -t_k \\log(y_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We seek and find the derivatives $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}V_{jk}}$, $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}C_{k}}$, $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}W_{ij}}$, $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}b_{j}}$ in that order, finding the derivative w.r.t. other intermediate values as necessary. We assume $n_y$ to be the number of classes (output layer outputs), $n_h$ to be the number of hidden units and $n_i$ to be the number of input units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first find the derivative w.r.t. $y_k$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "%%%%%%% dLdy_k\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k} &= \\frac{\\mathrm{d}}{\\mathrm{d}y_k} -t_k \\log(y_k) \\\\\n",
    "&= \\frac{-t_k}{y_k} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we find the derivative w.r.t. $y_k'$ using the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "%%%%%%% dLdy_k'\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} &= \\sum\\limits_{i=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i} \\frac{\\mathrm{d}y_i}{\\mathrm{d}y_k'} \\\\\n",
    "&= \\sum\\limits_{i=1}^{n_y} \\frac{-t_i}{y_i} \\frac{\\mathrm{d}y_i}{\\mathrm{d}y_k'} \\\\\n",
    "&\\textrm{But because $t_c = 1$ and all others are 0,} \\\\\n",
    "&= \\frac{-t_c}{y_c} \\frac{\\mathrm{d}y_c}{\\mathrm{d}y_k'} \\\\\n",
    "&= \\frac{-1}{y_c} \\frac{\\mathrm{d}}{\\mathrm{d}y_k'}\\left( \\frac{\\exp(y_c')}{\\sum\\limits_{j=1}^{n_y} \\exp(y_j')} \\right) \\\\\n",
    "&= \\frac{-1}{y_c} \\left( \\frac{\\sum\\limits_{j=1}^{n_y} \\exp(y_j') \\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') - \\exp(y_c') \\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\sum\\limits_{j=1}^{n_y} \\exp(y_j')}{\\left( \\sum\\limits_{j=1}^{N} \\exp(y_j') \\right)^2} \\right) \\\\\n",
    "&= \\frac{-1}{y_c} \\left( \\frac{\\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) \\sum\\limits_{j=1}^{n_y} \\left( \\exp(y_j') \\right) - \\exp(y_c') \\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_k')}{\\left( \\sum\\limits_{j=1}^{n_y} \\exp(y_j') \\right)^2} \\right) \\\\\n",
    "&= \\frac{-1}{y_c} \\left( \\frac{\\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) \\sum\\limits_{j=1}^{n_y} \\left( \\exp(y_j') \\right) - \\exp(y_c') \\exp(y_k')}{\\left( \\sum\\limits_{j=1}^{n_y} \\exp(y_j') \\right)^2} \\right) \\\\\n",
    "&\\textrm{We expand out the $\\frac{1}{y_c}$...} \\\\\n",
    "&= -\\left( \\frac{\\sum\\limits_{j=1}^{n_y} \\exp(y_j')}{\\exp(y_c')} \\right) \\left( \\frac{\\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) \\sum\\limits_{j=1}^{n_y} \\left( \\exp(y_j') \\right) - \\exp(y_c') \\exp(y_k')}{\\left( \\sum\\limits_{j=1}^{n_y} \\exp(y_j') \\right)^2} \\right) \\\\\n",
    "&= -\\left( \\frac{ \\left( \\frac{1}{\\exp(y_c')} \\right) \\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) \\sum\\limits_{j=1}^{n_y} \\left( \\exp(y_j') \\right) - \\exp(y_k')}{\\sum\\limits_{j=1}^{n_y} \\exp(y_j')} \\right) \\\\\n",
    "&= -\\left( \\frac{ \\left( \\frac{1}{\\exp(y_c')} \\right) \\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) \\sum\\limits_{j=1}^{n_y} \\left( \\exp(y_j') \\right)}{\\sum\\limits_{j=1}^{n_y} \\exp(y_j')} - \\frac{\\exp(y_k')}{\\sum\\limits_{j=1}^{n_y} \\exp(y_j')} \\right) \\\\\n",
    "&= -\\left( \\left( \\frac{1}{\\exp(y_c')} \\right) \\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) - y_k \\right) \\\\\n",
    "&\\textrm{We arrive at two cases: $k=c$ and $k \\ne c$. In the former,} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_c'} &= -\\left( \\left( \\frac{1}{\\exp(y_c')} \\right) \\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_c'} \\exp(y_c') \\right) - y_c \\right) \\\\\n",
    "&= -\\left( \\left( \\frac{1}{\\exp(y_c')} \\right) \\left( \\exp(y_c') \\right) - y_c \\right) \\\\\n",
    "&= -\\left( 1 - y_c \\right) \\\\\n",
    "&= y_c - 1 \\\\\n",
    "&\\textrm{In the latter,} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} &= -\\left( \\left( \\frac{1}{\\exp(y_c')} \\right) \\left(\\frac{\\mathrm{d}}{\\mathrm{d}y_k'} \\exp(y_c') \\right) - y_k \\right) \\\\\n",
    "&= -\\left( \\left( \\frac{1}{\\exp(y_c')} \\right)0 - y_k \\right) \\\\\n",
    "&= y_c - 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $V_{ij}$ using the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}V_{ij}} &= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}y_k'}{\\mathrm{d}V_{ij}} \\\\\n",
    "&\\textrm{But $\\mathbf{y'} = \\mathbf{V}\\mathbf{h} + \\mathbf{c}$, so the only $y_k'$ to interact with $V_{ij}$ is $y_i'$...} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i'} \\frac{\\mathrm{d}y_i'}{\\mathrm{d}V_{ij}} \\\\\n",
    "&\\textrm{and the only $h_a$ to interact with $V_{ij}$ is $h_j$.} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i'} h_{j} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $c_{i}$ using the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}c_{i}} &= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}y_k'}{\\mathrm{d}c_{i}} \\\\\n",
    "&\\textrm{Again, $\\mathbf{y'} = \\mathbf{V}\\mathbf{h} + \\mathbf{c}$, so the only $y_k'$ to interact with $c_{i}$ is $y_i'$...} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i'} \\frac{\\mathrm{d}y_i'}{\\mathrm{d}c_{i}} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i'} 1 \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_i'} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $h_{i}$ using the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_{i}} &= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}y_k'}{\\mathrm{d}h_{i}} \\\\\n",
    "&\\textrm{Again, $\\mathbf{y'} = \\mathbf{V}\\mathbf{h} + \\mathbf{c}$, and thus we have} \\\\\n",
    "&= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}}{\\mathrm{d}h_{i}} \\left( c_{k} + \\sum\\limits_{j=1}^{n_h} V_{kj}h_{j} \\right) \\\\\n",
    "&= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}}{\\mathrm{d}h_{i}} \\left( V_{ki}h_{i} \\right) \\\\\n",
    "&= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} V_{ki} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $h_{i}'$. It is an elementwise derivative of the sigmoid.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_{i}'} &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i} \\frac{\\mathrm{d}h_i}{\\mathrm{d}h_{i}'} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i} \\frac{\\mathrm{d}}{\\mathrm{d}h_{i}'} \\left( \\sigma(h_i') \\right) \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i} \\sigma(h_i')(1-\\sigma(h_i')) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $W_{ij}$, almost duplicating the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}W_{ij}} &= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_k'} \\frac{\\mathrm{d}h_k'}{\\mathrm{d}W_{ij}} \\\\\n",
    "&\\textrm{But $\\mathbf{h'} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, so the only $h_k'$ to interact with $W_{ij}$ is $h_i'$...} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i'} \\frac{\\mathrm{d}h_i'}{\\mathrm{d}W_{ij}} \\\\\n",
    "&\\textrm{and the only $x_a$ to interact with $W_{ij}$ is $x_j$.} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i'} x_{j} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the derivative w.r.t. $b_{i}$, almost duplicating the work above:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}b_{i}} &= \\sum\\limits_{k=1}^{n_y} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}y_k'} \\frac{\\mathrm{d}y_k'}{\\mathrm{d}b_{i}} \\\\\n",
    "&\\textrm{Again, $\\mathbf{h'} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, so the only $h_k'$ to interact with $b_{i}$ is $h_i'$...} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i'} \\frac{\\mathrm{d}h_i'}{\\mathrm{d}b_{i}} \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i'} 1 \\\\\n",
    "&= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}h_i'} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Using what you derived in step 1, find an expression for the derivative of the loss function with respect to parameter vectors and matrices $\\mathbf{W}$, $\\mathbf{b}$, $\\mathbf{V}$, $\\mathbf{c}$. This is a good thing to do, because NumPy has abstractions to represent dot products between vectors, vector-matrix products, elementwise operations, etc. that rely on heavily optimized linear algebra libraries.  Coding up all these operations using for-loops would be both inefficient and tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the \"correct\" output label's index is $c$, i.e. that $t_c = 1$ and $\\forall i \\ne c, t_i = 0$. We denote the elementwise product as $\\odot$. The above results in matrix form are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{y}}  &= \\frac{-\\mathbf{t}}{\\mathbf{y}} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{y'}} &= \\mathbf{y} - \\mathbf{t} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{V}}  &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{y'}} \\mathbf{h}^\\mathrm{T} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{c}}  &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{y'}} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{h}}  &= \\mathbf{V}^\\mathrm{T} \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{y'}}\\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{h'}} &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{h}} \\odot  \\sigma(\\mathbf{h'}) \\odot (1-\\sigma(\\mathbf{h'})) \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{W}}  &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{h'}} \\mathbf{x}^\\mathrm{T} \\\\\n",
    "\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{b}}  &= \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\mathbf{h'}} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Implement the code necessary to compute gradients using the expressions derived in step 2 for a pair of feature and target vectors $\\mathbf{x}$ and $\\mathbf{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utilities\n",
    "\n",
    "#\n",
    "# Sigmoid\n",
    "#\n",
    "\n",
    "def sigmoid(x, axis=-1):\n",
    "    #For reasons of numerical stability we avoid computing np.exp(for highly negative values.)\n",
    "    absx     = np.abs(x)\n",
    "    absxsigm = 1/(1 + np.exp(-absx))\n",
    "    \n",
    "    #At this point, in places where x<0, we've mirrored about y=1/2: The function is going up instead of down.\n",
    "    #So we reverse this by conditionally multiplying only those places' result by -1, bringing them to the range\n",
    "    #(-1, -1/2), and then we add +1 to boost that to (0, 1/2).\n",
    "    \n",
    "    signx    = np.where(x<0, -1, +1)\n",
    "    biasx    = np.where(x<0,  1,  0)\n",
    "    return absxsigm*signx + biasx\n",
    "\n",
    "#\n",
    "# Softmax\n",
    "#\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    #For reasons of numerical stability we reduce the largest input x in any series to 0. Since we then take\n",
    "    #a ratio, the scale factors introduced cancel out, and thus it's not necessary to undo this shift.\n",
    "    \n",
    "    maxx    = np.expand_dims(np.max(x,    axis), axis) #Makes broadcast work\n",
    "    expx    = np.exp(x - maxx)\n",
    "    sumexpx = np.expand_dims(np.sum(expx, axis), axis) #Makes broadcast work\n",
    "    return expx/sumexpx\n",
    "    \n",
    "\n",
    "\n",
    "#Class\n",
    "\n",
    "#\n",
    "# Multilayer Perceptron\n",
    "#\n",
    "\n",
    "class MLP:\n",
    "    #\n",
    "    # Constructor\n",
    "    #\n",
    "    \n",
    "    def __init__(self, ni,nh,no):\n",
    "        #Save integer arguments \n",
    "        self.ni = int(ni)\n",
    "        self.nh = int(nh)\n",
    "        self.no = int(no)\n",
    "        \n",
    "        #Allocate and initialize randomly parameters\n",
    "        self.W  = np.random.normal(0, 1/np.sqrt(nh*ni), (nh,ni))   # W is (nh x ni)\n",
    "        self.b  = np.random.normal(0, 1/np.sqrt(nh),    (nh,))     # b is (nh)\n",
    "        self.V  = np.random.normal(0, 1/np.sqrt(no*nh), (no,nh))   # V is (no x nh)\n",
    "        self.c  = np.random.normal(0, 1/np.sqrt(no),    (no,))     # c is (no)\n",
    "        \n",
    "        #And velocities\n",
    "        self.vW = np.zeros_like(self.W)   # W is (nh x ni)\n",
    "        self.vb = np.zeros_like(self.b)   # b is (nh)\n",
    "        self.vV = np.zeros_like(self.V)   # V is (no x nh)\n",
    "        self.vc = np.zeros_like(self.c)   # c is (no)\n",
    "    \n",
    "    #\n",
    "    # Forward pass.\n",
    "    #\n",
    "    # Compute eveything up to and including the loss.\n",
    "    #\n",
    "    \n",
    "    def fprop(self, x, t):\n",
    "        self.x  = x\n",
    "        # self.hp = np.einsum(\"hi,...i->...h\", self.W, self.x) + self.b\n",
    "        self.hp = np.dot(self.x, self.W.T) + self.b\n",
    "        self.h  = sigmoid(self.hp)\n",
    "        # self.yp = np.einsum(\"oh,...h->...o\", self.V, self.h) + self.c\n",
    "        self.yp = np.dot(self.h, self.V.T) + self.c\n",
    "        self.y  = softmax(self.yp)\n",
    "        self.t  = t\n",
    "        self.L  = np.sum(-self.t*np.log(self.y), axis=-1)\n",
    "        \n",
    "        return self.L\n",
    "    \n",
    "    #\n",
    "    # Backprop pass, exact.\n",
    "    #\n",
    "    \n",
    "    def bprop(self, x, t):\n",
    "        self.fprop(x, t)\n",
    "        \n",
    "        #Compute for the example(s) the derivative(s) with respect to all parameters.\n",
    "        self.dLdyp = self.y - self.t\n",
    "        self.dLdV  = np.einsum(\"...o,...h->...oh\", self.dLdyp, self.h)\n",
    "        self.dLdc  = self.dLdyp\n",
    "        # self.dLdh  = np.einsum(\"...o,oh->...h\", self.dLdyp, self.V)\n",
    "        self.dLdh  = np.dot(self.dLdyp, self.V)\n",
    "        self.dLdhp = self.dLdh * sigmoid(self.hp) * (1-sigmoid(self.hp))\n",
    "        self.dLdW  = np.einsum(\"...h,...i->...hi\", self.dLdhp, self.x)\n",
    "        self.dLdb  = self.dLdhp\n",
    "        \n",
    "        #Collapse the arbitrary-dimensional number of examples to the dimensions of the parameters\n",
    "        #by averaging over all examples.\n",
    "        self.dLdyp = np.mean(self.dLdyp, axis=tuple(range(0, self.dLdyp.ndim-1)))\n",
    "        self.dLdV  = np.mean(self.dLdV,  axis=tuple(range(0, self.dLdV .ndim-2)))\n",
    "        self.dLdc  = np.mean(self.dLdc,  axis=tuple(range(0, self.dLdc .ndim-1)))\n",
    "        self.dLdh  = np.mean(self.dLdh,  axis=tuple(range(0, self.dLdh .ndim-1)))\n",
    "        self.dLdhp = np.mean(self.dLdhp, axis=tuple(range(0, self.dLdhp.ndim-1)))\n",
    "        self.dLdW  = np.mean(self.dLdW,  axis=tuple(range(0, self.dLdW .ndim-2)))\n",
    "        self.dLdb  = np.mean(self.dLdb,  axis=tuple(range(0, self.dLdb .ndim-1)))\n",
    "    \n",
    "    #\n",
    "    # Backprop pass, finite-difference.\n",
    "    #\n",
    "    \n",
    "    def bpropfd(self, x, t, eps=1e-5):\n",
    "        #\n",
    "        # We iterate over each parameter and run two forwardprops, one with -eps and one with +eps.\n",
    "        # The procedure for a parameter test is to:\n",
    "        # \n",
    "        #    1. Save the old value,\n",
    "        #    2. Run test with -eps\n",
    "        #    3. Save loss with -eps\n",
    "        #    4. Run test with +eps\n",
    "        #    5. Save loss with +eps\n",
    "        #    6. Restore old value.\n",
    "        #    7. Compute finite difference as (plusEpsL - minusEpsL)/(2*eps)\n",
    "        #\n",
    "        \n",
    "        self.fd_dLdV  = np.empty_like(self.V)\n",
    "        self.fd_dLdc  = np.empty_like(self.c)\n",
    "        self.fd_dLdW  = np.empty_like(self.W)\n",
    "        self.fd_dLdb  = np.empty_like(self.b)\n",
    "        \n",
    "        #\n",
    "        # W\n",
    "        #\n",
    "        \n",
    "        for i in xrange(self.W.shape[0]):\n",
    "            for j in xrange(self.W.shape[1]):\n",
    "                # 1.\n",
    "                old         = self.W[i,j]\n",
    "                # 2.\n",
    "                self.W[i,j] = old - eps\n",
    "                # 3.\n",
    "                minusEpsL   = np.mean(self.fprop(x, t))\n",
    "                # 4.\n",
    "                self.W[i,j] = old + eps\n",
    "                # 5.\n",
    "                plusEpsL    = np.mean(self.fprop(x, t))\n",
    "                # 6.\n",
    "                self.W[i,j] = old\n",
    "                # 7.\n",
    "                self.fd_dLdW[i,j] = (plusEpsL - minusEpsL)/(2*eps)\n",
    "        \n",
    "        #\n",
    "        # b\n",
    "        #\n",
    "        \n",
    "        for i in xrange(self.b.shape[0]):\n",
    "            # 1.\n",
    "            old         = self.b[i]\n",
    "            # 2.\n",
    "            self.b[i]   = old - eps\n",
    "            # 3.\n",
    "            minusEpsL   = np.mean(self.fprop(x, t))\n",
    "            # 4.\n",
    "            self.b[i]   = old + eps\n",
    "            # 5.\n",
    "            plusEpsL    = np.mean(self.fprop(x, t))\n",
    "            # 6.\n",
    "            self.b[i]   = old\n",
    "            # 7.\n",
    "            self.fd_dLdb[i] = (plusEpsL - minusEpsL)/(2*eps)\n",
    "        \n",
    "        #\n",
    "        # V\n",
    "        #\n",
    "        \n",
    "        for i in xrange(self.V.shape[0]):\n",
    "            for j in xrange(self.V.shape[1]):\n",
    "                # 1.\n",
    "                old         = self.V[i,j]\n",
    "                # 2.\n",
    "                self.V[i,j] = old - eps\n",
    "                # 3.\n",
    "                minusEpsL   = np.mean(self.fprop(x, t))\n",
    "                # 4.\n",
    "                self.V[i,j] = old + eps\n",
    "                # 5.\n",
    "                plusEpsL    = np.mean(self.fprop(x, t))\n",
    "                # 6.\n",
    "                self.V[i,j] = old\n",
    "                # 7.\n",
    "                self.fd_dLdV[i,j] = (plusEpsL - minusEpsL)/(2*eps)\n",
    "        \n",
    "        #\n",
    "        # c\n",
    "        #\n",
    "        \n",
    "        for i in xrange(self.c.shape[0]):\n",
    "            # 1.\n",
    "            old         = self.c[i]\n",
    "            # 2.\n",
    "            self.c[i]   = old - eps\n",
    "            # 3.\n",
    "            minusEpsL   = np.mean(self.fprop(x, t))\n",
    "            # 4.\n",
    "            self.c[i]   = old + eps\n",
    "            # 5.\n",
    "            plusEpsL    = np.mean(self.fprop(x, t))\n",
    "            # 6.\n",
    "            self.c[i]   = old\n",
    "            # 7.\n",
    "            self.fd_dLdc[i] = (plusEpsL - minusEpsL)/(2*eps)\n",
    "        \n",
    "        #\n",
    "        # Assert closeness\n",
    "        #\n",
    "        \n",
    "        self.bprop(x,t)\n",
    "        np.testing.assert_allclose(self.dLdW, self.fd_dLdW)\n",
    "        np.testing.assert_allclose(self.dLdb, self.fd_dLdb)\n",
    "        np.testing.assert_allclose(self.dLdV, self.fd_dLdV)\n",
    "        np.testing.assert_allclose(self.dLdc, self.fd_dLdc)\n",
    "    \n",
    "    #\n",
    "    # Update.\n",
    "    #\n",
    "    \n",
    "    def update(self, learningrate, momentum=0.9):\n",
    "        learningrate = float(learningrate)\n",
    "        momentum     = float(momentum)\n",
    "        \n",
    "        self.vW = momentum*self.vW + self.dLdW\n",
    "        self.vb = momentum*self.vb + self.dLdb\n",
    "        self.vV = momentum*self.vV + self.dLdV\n",
    "        self.vc = momentum*self.vc + self.dLdc\n",
    "        \n",
    "        self.W -= learningrate * self.vW\n",
    "        self.b -= learningrate * self.vb\n",
    "        self.V -= learningrate * self.vV\n",
    "        self.c -= learningrate * self.vc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Using the finite differences method, implement a numerical version of the code that computes the gradients, and compare the analytical gradients and the numerical gradients using a toy example (e.g. a random pair of feature and target vectors for an MLP with 10 input dimensions, 5 hidden units and 2 categories). A mismatch between the two indicates an error in the implementation of the analytical gradients. The `numpy.testing.assert_allclose` method will be handy for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create MLP object\n",
    "ni = 10\n",
    "nh =  5\n",
    "no =  2\n",
    "mlp = MLP(ni,nh,no)\n",
    "\n",
    "#Random example\n",
    "x = np.random.normal(size=(ni,))\n",
    "c = np.random.randint(0, no)\n",
    "t = np.eye(no)[c]\n",
    "\n",
    "#Run bpropfd, which internally also runs exact bprop and performs the assertions.\n",
    "mlp.bpropfd(x,t,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Generalize your code to work on minibatches of data. Make sure that numerical and analytical gradients still check out on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We already support batch processing in the code above. Let's use batch size 128.\n",
    "B  = 128\n",
    "\n",
    "#Create MLP object\n",
    "ni = 10\n",
    "nh =  5\n",
    "no =  2\n",
    "mlp = MLP(ni,nh,no)\n",
    "\n",
    "#Random examples\n",
    "x = np.random.normal(size=(B, ni))\n",
    "c = np.random.randint(0, no, (B,))\n",
    "t = np.eye(no)[c]\n",
    "\n",
    "#Run bpropfd, which internally also runs exact bprop and performs the assertions.\n",
    "mlp.bpropfd(x,t,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Implement a simple minibatch SGD loop and train your MLP on MNIST. You should be able to achieve under 5% error rate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0:    TrainErr   90.10%    ValidErr   90.33%     Time:   0.63s\n",
      "Epoch     1:    TrainErr   10.30%    ValidErr    9.25%     Time:  30.58s\n",
      "Epoch     2:    TrainErr    8.58%    ValidErr    7.98%     Time:  31.68s\n",
      "Epoch     3:    TrainErr    7.55%    ValidErr    6.98%     Time:  30.60s\n",
      "Epoch     4:    TrainErr    6.76%    ValidErr    6.11%     Time:  31.27s\n",
      "Epoch     5:    TrainErr    6.01%    ValidErr    5.43%     Time:  32.52s\n",
      "Epoch     6:    TrainErr    5.34%    ValidErr    4.86%     Time:  32.92s\n",
      "Epoch     7:    TrainErr    4.80%    ValidErr    4.41%     Time:  31.39s\n",
      "Epoch     8:    TrainErr    4.31%    ValidErr    4.11%     Time:  30.52s\n",
      "Epoch     9:    TrainErr    3.97%    ValidErr    3.91%     Time:  30.39s\n",
      "Epoch    10:    TrainErr    3.64%    ValidErr    3.67%     Time:  32.14s\n",
      "Epoch    11:    TrainErr    3.29%    ValidErr    3.50%     Time:  31.00s\n",
      "Epoch    12:    TrainErr    3.03%    ValidErr    3.37%     Time:  32.00s\n",
      "Epoch    13:    TrainErr    2.86%    ValidErr    3.16%     Time:  32.26s\n",
      "Epoch    14:    TrainErr    2.68%    ValidErr    3.00%     Time:  32.37s\n",
      "Epoch    15:    TrainErr    2.52%    ValidErr    2.94%     Time:  32.39s\n",
      "Epoch    16:    TrainErr    2.34%    ValidErr    2.91%     Time:  34.86s\n",
      "Epoch    17:    TrainErr    2.18%    ValidErr    2.86%     Time:  30.53s\n",
      "Epoch    18:    TrainErr    2.05%    ValidErr    2.75%     Time:  30.33s\n",
      "Epoch    19:    TrainErr    1.90%    ValidErr    2.74%     Time:  30.85s\n",
      "Epoch    20:    TrainErr    1.81%    ValidErr    2.67%     Time:  35.01s\n",
      "Epoch    21:    TrainErr    1.70%    ValidErr    2.63%     Time:  29.30s\n",
      "Epoch    22:    TrainErr    1.62%    ValidErr    2.59%     Time:  29.38s\n",
      "Epoch    23:    TrainErr    1.54%    ValidErr    2.52%     Time:  32.23s\n",
      "Epoch    24:    TrainErr    1.47%    ValidErr    2.49%     Time:  30.11s\n",
      "Epoch    25:    TrainErr    1.39%    ValidErr    2.44%     Time:  29.95s\n",
      "Epoch    26:    TrainErr    1.30%    ValidErr    2.38%     Time:  30.80s\n",
      "Epoch    27:    TrainErr    1.21%    ValidErr    2.37%     Time:  30.57s\n",
      "Epoch    28:    TrainErr    1.14%    ValidErr    2.33%     Time:  31.93s\n",
      "Epoch    29:    TrainErr    1.09%    ValidErr    2.34%     Time:  29.78s\n",
      "Epoch    30:    TrainErr    1.04%    ValidErr    2.33%     Time:  29.92s\n",
      "Epoch    31:    TrainErr    0.98%    ValidErr    2.35%     Time:  30.21s\n",
      "Epoch    32:    TrainErr    0.91%    ValidErr    2.33%     Time:  29.71s\n",
      "Epoch    33:    TrainErr    0.86%    ValidErr    2.35%     Time:  29.69s\n",
      "Epoch    34:    TrainErr    0.81%    ValidErr    2.34%     Time:  29.37s\n",
      "Epoch    35:    TrainErr    0.76%    ValidErr    2.30%     Time:  29.30s\n",
      "Epoch    36:    TrainErr    0.71%    ValidErr    2.30%     Time:  29.17s\n",
      "Epoch    37:    TrainErr    0.66%    ValidErr    2.28%     Time:  30.68s\n",
      "Epoch    38:    TrainErr    0.62%    ValidErr    2.29%     Time:  31.25s\n",
      "Epoch    39:    TrainErr    0.58%    ValidErr    2.29%     Time:  30.42s\n",
      "Epoch    40:    TrainErr    0.54%    ValidErr    2.27%     Time:  29.77s\n",
      "Epoch    41:    TrainErr    0.51%    ValidErr    2.25%     Time:  29.53s\n",
      "Epoch    42:    TrainErr    0.47%    ValidErr    2.25%     Time:  30.66s\n",
      "Epoch    43:    TrainErr    0.43%    ValidErr    2.25%     Time:  29.83s\n",
      "Epoch    44:    TrainErr    0.39%    ValidErr    2.24%     Time:  30.45s\n",
      "Epoch    45:    TrainErr    0.37%    ValidErr    2.22%     Time:  29.41s\n",
      "Epoch    46:    TrainErr    0.35%    ValidErr    2.20%     Time:  28.88s\n",
      "Epoch    47:    TrainErr    0.33%    ValidErr    2.20%     Time:  29.55s\n",
      "Epoch    48:    TrainErr    0.32%    ValidErr    2.21%     Time:  30.61s\n",
      "Epoch    49:    TrainErr    0.31%    ValidErr    2.21%     Time:  30.15s\n",
      "Epoch    50:    TrainErr    0.28%    ValidErr    2.20%     Time:  29.49s\n",
      "Epoch    51:    TrainErr    0.26%    ValidErr    2.20%     Time:  29.41s\n",
      "Epoch    52:    TrainErr    0.25%    ValidErr    2.18%     Time:  30.15s\n",
      "Epoch    53:    TrainErr    0.23%    ValidErr    2.16%     Time:  30.00s\n",
      "Epoch    54:    TrainErr    0.22%    ValidErr    2.15%     Time:  29.78s\n",
      "Epoch    55:    TrainErr    0.21%    ValidErr    2.13%     Time:  32.32s\n",
      "Epoch    56:    TrainErr    0.18%    ValidErr    2.12%     Time:  31.10s\n",
      "Epoch    57:    TrainErr    0.17%    ValidErr    2.12%     Time:  31.90s\n",
      "Epoch    58:    TrainErr    0.15%    ValidErr    2.08%     Time:  29.35s\n",
      "Epoch    59:    TrainErr    0.14%    ValidErr    2.09%     Time:  29.41s\n",
      "Epoch    60:    TrainErr    0.13%    ValidErr    2.08%     Time:  29.55s\n",
      "Epoch    61:    TrainErr    0.12%    ValidErr    2.07%     Time:  29.48s\n",
      "Epoch    62:    TrainErr    0.11%    ValidErr    2.10%     Time:  31.45s\n",
      "Epoch    63:    TrainErr    0.10%    ValidErr    2.10%     Time:  29.55s\n",
      "Epoch    64:    TrainErr    0.10%    ValidErr    2.10%     Time:  29.63s\n",
      "Epoch    65:    TrainErr    0.09%    ValidErr    2.08%     Time:  30.21s\n",
      "Epoch    66:    TrainErr    0.09%    ValidErr    2.08%     Time:  31.63s\n",
      "Epoch    67:    TrainErr    0.08%    ValidErr    2.08%     Time:  32.71s\n",
      "Epoch    68:    TrainErr    0.07%    ValidErr    2.08%     Time:  30.45s\n",
      "Epoch    69:    TrainErr    0.07%    ValidErr    2.08%     Time:  29.87s\n",
      "Epoch    70:    TrainErr    0.06%    ValidErr    2.09%     Time:  29.01s\n",
      "Epoch    71:    TrainErr    0.06%    ValidErr    2.09%     Time:  31.95s\n",
      "Epoch    72:    TrainErr    0.06%    ValidErr    2.09%     Time:  31.14s\n",
      "Epoch    73:    TrainErr    0.05%    ValidErr    2.09%     Time:  29.93s\n",
      "Epoch    74:    TrainErr    0.05%    ValidErr    2.08%     Time:  31.57s\n",
      "Epoch    75:    TrainErr    0.05%    ValidErr    2.08%     Time:  30.67s\n",
      "Epoch    76:    TrainErr    0.04%    ValidErr    2.09%     Time:  29.81s\n",
      "Epoch    77:    TrainErr    0.04%    ValidErr    2.08%     Time:  31.20s\n",
      "Epoch    78:    TrainErr    0.04%    ValidErr    2.06%     Time:  31.40s\n",
      "Epoch    79:    TrainErr    0.04%    ValidErr    2.06%     Time:  30.00s\n",
      "Epoch    80:    TrainErr    0.04%    ValidErr    2.05%     Time:  29.84s\n",
      "Epoch    81:    TrainErr    0.04%    ValidErr    2.04%     Time:  29.88s\n",
      "Epoch    82:    TrainErr    0.03%    ValidErr    2.04%     Time:  29.52s\n",
      "Epoch    83:    TrainErr    0.03%    ValidErr    2.04%     Time:  29.59s\n",
      "Epoch    84:    TrainErr    0.03%    ValidErr    2.04%     Time:  29.61s\n",
      "Epoch    85:    TrainErr    0.03%    ValidErr    2.04%     Time:  29.67s\n",
      "Epoch    86:    TrainErr    0.02%    ValidErr    2.04%     Time:  29.62s\n",
      "Epoch    87:    TrainErr    0.02%    ValidErr    2.04%     Time:  32.03s\n",
      "Epoch    88:    TrainErr    0.02%    ValidErr    2.05%     Time:  31.94s\n",
      "Epoch    89:    TrainErr    0.02%    ValidErr    2.05%     Time:  34.67s\n",
      "Epoch    90:    TrainErr    0.02%    ValidErr    2.05%     Time:  31.44s\n",
      "Epoch    91:    TrainErr    0.02%    ValidErr    2.04%     Time:  29.52s\n",
      "Epoch    92:    TrainErr    0.02%    ValidErr    2.04%     Time:  29.77s\n",
      "Epoch    93:    TrainErr    0.02%    ValidErr    2.03%     Time:  31.09s\n",
      "Epoch    94:    TrainErr    0.02%    ValidErr    2.03%     Time:  32.21s\n",
      "Epoch    95:    TrainErr    0.02%    ValidErr    2.03%     Time:  31.29s\n",
      "Epoch    96:    TrainErr    0.02%    ValidErr    2.03%     Time:  30.13s\n",
      "Epoch    97:    TrainErr    0.02%    ValidErr    2.03%     Time:  30.04s\n",
      "Epoch    98:    TrainErr    0.02%    ValidErr    2.03%     Time:  30.09s\n",
      "Epoch    99:    TrainErr    0.02%    ValidErr    2.03%     Time:  30.58s\n",
      "Epoch   100:    TrainErr    0.01%    ValidErr    2.00%     Time:  31.82s\n",
      "Epoch   101:    TrainErr    0.01%    ValidErr    1.99%     Time:  30.20s\n",
      "Epoch   102:    TrainErr    0.01%    ValidErr    1.98%     Time:  29.99s\n",
      "Epoch   103:    TrainErr    0.01%    ValidErr    1.98%     Time:  29.31s\n",
      "Epoch   104:    TrainErr    0.01%    ValidErr    1.98%     Time:  29.62s\n",
      "Epoch   105:    TrainErr    0.01%    ValidErr    1.98%     Time:  34.35s\n",
      "Epoch   106:    TrainErr    0.01%    ValidErr    1.98%     Time:  31.70s\n",
      "Epoch   107:    TrainErr    0.01%    ValidErr    2.00%     Time:  30.85s\n",
      "Epoch   108:    TrainErr    0.01%    ValidErr    2.00%     Time:  30.90s\n",
      "Epoch   109:    TrainErr    0.01%    ValidErr    2.00%     Time:  29.45s\n",
      "Epoch   110:    TrainErr    0.01%    ValidErr    2.00%     Time:  29.35s\n",
      "Epoch   111:    TrainErr    0.01%    ValidErr    2.00%     Time:  29.75s\n",
      "Epoch   112:    TrainErr    0.00%    ValidErr    1.99%     Time:  32.24s\n",
      "Epoch   113:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.46s\n",
      "Epoch   114:    TrainErr    0.00%    ValidErr    1.99%     Time:  31.74s\n",
      "Epoch   115:    TrainErr    0.00%    ValidErr    1.99%     Time:  32.25s\n",
      "Epoch   116:    TrainErr    0.00%    ValidErr    1.99%     Time:  32.17s\n",
      "Epoch   117:    TrainErr    0.00%    ValidErr    1.99%     Time:  32.28s\n",
      "Epoch   118:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.52s\n",
      "Epoch   119:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.39s\n",
      "Epoch   120:    TrainErr    0.00%    ValidErr    1.99%     Time:  31.07s\n",
      "Epoch   121:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.71s\n",
      "Epoch   122:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.80s\n",
      "Epoch   123:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.94s\n",
      "Epoch   124:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.80s\n",
      "Epoch   125:    TrainErr    0.00%    ValidErr    1.98%     Time:  29.63s\n",
      "Epoch   126:    TrainErr    0.00%    ValidErr    1.98%     Time:  32.21s\n",
      "Epoch   127:    TrainErr    0.00%    ValidErr    1.98%     Time:  33.87s\n",
      "Epoch   128:    TrainErr    0.00%    ValidErr    1.96%     Time:  30.59s\n",
      "Epoch   129:    TrainErr    0.00%    ValidErr    1.96%     Time:  30.69s\n",
      "Epoch   130:    TrainErr    0.00%    ValidErr    1.96%     Time:  31.11s\n",
      "Epoch   131:    TrainErr    0.00%    ValidErr    1.96%     Time:  31.88s\n",
      "Epoch   132:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.08s\n",
      "Epoch   133:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.53s\n",
      "Epoch   134:    TrainErr    0.00%    ValidErr    1.97%     Time:  33.62s\n",
      "Epoch   135:    TrainErr    0.00%    ValidErr    1.97%     Time:  32.58s\n",
      "Epoch   136:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.71s\n",
      "Epoch   137:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.02s\n",
      "Epoch   138:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.22s\n",
      "Epoch   139:    TrainErr    0.00%    ValidErr    1.97%     Time:  34.69s\n",
      "Epoch   140:    TrainErr    0.00%    ValidErr    1.97%     Time:  39.02s\n",
      "Epoch   141:    TrainErr    0.00%    ValidErr    1.97%     Time:  37.50s\n",
      "Epoch   142:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.58s\n",
      "Epoch   143:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.16s\n",
      "Epoch   144:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.87s\n",
      "Epoch   145:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.76s\n",
      "Epoch   146:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.29s\n",
      "Epoch   147:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.62s\n",
      "Epoch   148:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.44s\n",
      "Epoch   149:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.73s\n",
      "Epoch   150:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.98s\n",
      "Epoch   151:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.91s\n",
      "Epoch   152:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.06s\n",
      "Epoch   153:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.05s\n",
      "Epoch   154:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.81s\n",
      "Epoch   155:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.19s\n",
      "Epoch   156:    TrainErr    0.00%    ValidErr    1.98%     Time:  38.39s\n",
      "Epoch   157:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.58s\n",
      "Epoch   158:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.45s\n",
      "Epoch   159:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.66s\n",
      "Epoch   160:    TrainErr    0.00%    ValidErr    1.97%     Time:  30.57s\n",
      "Epoch   161:    TrainErr    0.00%    ValidErr    1.97%     Time:  31.18s\n",
      "Epoch   162:    TrainErr    0.00%    ValidErr    1.98%     Time:  31.54s\n",
      "Epoch   163:    TrainErr    0.00%    ValidErr    1.98%     Time:  29.87s\n",
      "Epoch   164:    TrainErr    0.00%    ValidErr    1.98%     Time:  31.44s\n",
      "Epoch   165:    TrainErr    0.00%    ValidErr    1.98%     Time:  31.41s\n",
      "Epoch   166:    TrainErr    0.00%    ValidErr    1.98%     Time:  33.16s\n",
      "Epoch   167:    TrainErr    0.00%    ValidErr    1.98%     Time:  31.63s\n",
      "Epoch   168:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.66s\n",
      "Epoch   169:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.69s\n",
      "Epoch   170:    TrainErr    0.00%    ValidErr    1.98%     Time:  33.57s\n",
      "Epoch   171:    TrainErr    0.00%    ValidErr    1.98%     Time:  32.83s\n",
      "Epoch   172:    TrainErr    0.00%    ValidErr    1.98%     Time:  30.30s\n",
      "Epoch   173:    TrainErr    0.00%    ValidErr    1.98%     Time:  31.48s\n",
      "Epoch   174:    TrainErr    0.00%    ValidErr    1.98%     Time:  29.98s\n",
      "Epoch   175:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.65s\n",
      "Epoch   176:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.66s\n",
      "Epoch   177:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.23s\n",
      "Epoch   178:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.12s\n",
      "Epoch   179:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.68s\n",
      "Epoch   180:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.70s\n",
      "Epoch   181:    TrainErr    0.00%    ValidErr    2.00%     Time:  29.64s\n",
      "Epoch   182:    TrainErr    0.00%    ValidErr    2.00%     Time:  29.26s\n",
      "Epoch   183:    TrainErr    0.00%    ValidErr    1.99%     Time:  31.51s\n",
      "Epoch   184:    TrainErr    0.00%    ValidErr    1.99%     Time:  39.02s\n",
      "Epoch   185:    TrainErr    0.00%    ValidErr    1.99%     Time:  32.67s\n",
      "Epoch   186:    TrainErr    0.00%    ValidErr    1.99%     Time:  30.22s\n",
      "Epoch   187:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.26s\n",
      "Epoch   188:    TrainErr    0.00%    ValidErr    1.99%     Time:  29.58s\n",
      "Epoch   189:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.68s\n",
      "Epoch   190:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.56s\n",
      "Epoch   191:    TrainErr    0.00%    ValidErr    1.97%     Time:  35.97s\n",
      "Epoch   192:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.70s\n",
      "Epoch   193:    TrainErr    0.00%    ValidErr    1.97%     Time:  32.91s\n",
      "Epoch   194:    TrainErr    0.00%    ValidErr    1.96%     Time:  37.73s\n",
      "Epoch   195:    TrainErr    0.00%    ValidErr    1.96%     Time:  37.68s\n",
      "Epoch   196:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.57s\n",
      "Epoch   197:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.61s\n",
      "Epoch   198:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.63s\n",
      "Epoch   199:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.56s\n",
      "Epoch   200:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.66s\n",
      "Epoch   201:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.57s\n",
      "Epoch   202:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.63s\n",
      "Epoch   203:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.56s\n",
      "Epoch   204:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.93s\n",
      "Epoch   205:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.54s\n",
      "Epoch   206:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.66s\n",
      "Epoch   207:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.18s\n",
      "Epoch   208:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.50s\n",
      "Epoch   209:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.62s\n",
      "Epoch   210:    TrainErr    0.00%    ValidErr    1.96%     Time:  31.99s\n",
      "Epoch   211:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.54s\n",
      "Epoch   212:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.49s\n",
      "Epoch   213:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.15s\n",
      "Epoch   214:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.58s\n",
      "Epoch   215:    TrainErr    0.00%    ValidErr    1.96%     Time:  31.31s\n",
      "Epoch   216:    TrainErr    0.00%    ValidErr    1.96%     Time:  30.26s\n",
      "Epoch   217:    TrainErr    0.00%    ValidErr    1.96%     Time:  33.79s\n",
      "Epoch   218:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.56s\n",
      "Epoch   219:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.71s\n",
      "Epoch   220:    TrainErr    0.00%    ValidErr    1.96%     Time:  29.44s\n",
      "Epoch   221:    TrainErr    0.00%    ValidErr    1.96%     Time:  30.86s\n",
      "Epoch   222:    TrainErr    0.00%    ValidErr    1.97%     Time:  29.78s\n",
      "Epoch   223:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.84s\n",
      "Epoch   224:    TrainErr    0.00%    ValidErr    1.95%     Time:  36.06s\n",
      "Epoch   225:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.28s\n",
      "Epoch   226:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.55s\n",
      "Epoch   227:    TrainErr    0.00%    ValidErr    1.95%     Time:  33.35s\n",
      "Epoch   228:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.86s\n",
      "Epoch   229:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.10s\n",
      "Epoch   230:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.86s\n",
      "Epoch   231:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.47s\n",
      "Epoch   232:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.53s\n",
      "Epoch   233:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.80s\n",
      "Epoch   234:    TrainErr    0.00%    ValidErr    1.94%     Time:  32.06s\n",
      "Epoch   235:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.66s\n",
      "Epoch   236:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.51s\n",
      "Epoch   237:    TrainErr    0.00%    ValidErr    1.95%     Time:  35.09s\n",
      "Epoch   238:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.36s\n",
      "Epoch   239:    TrainErr    0.00%    ValidErr    1.95%     Time:  29.77s\n",
      "Epoch   240:    TrainErr    0.00%    ValidErr    1.95%     Time:  34.48s\n",
      "Epoch   241:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.61s\n",
      "Epoch   242:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.69s\n",
      "Epoch   243:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.43s\n",
      "Epoch   244:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.84s\n",
      "Epoch   245:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.84s\n",
      "Epoch   246:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.97s\n",
      "Epoch   247:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.97s\n",
      "Epoch   248:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.44s\n",
      "Epoch   249:    TrainErr    0.00%    ValidErr    1.95%     Time:  35.15s\n",
      "Epoch   250:    TrainErr    0.00%    ValidErr    1.95%     Time:  36.85s\n",
      "Epoch   251:    TrainErr    0.00%    ValidErr    1.95%     Time:  34.97s\n",
      "Epoch   252:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.01s\n",
      "Epoch   253:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.30s\n",
      "Epoch   254:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.99s\n",
      "Epoch   255:    TrainErr    0.00%    ValidErr    1.95%     Time:  33.66s\n",
      "Epoch   256:    TrainErr    0.00%    ValidErr    1.95%     Time:  33.31s\n",
      "Epoch   257:    TrainErr    0.00%    ValidErr    1.95%     Time:  36.05s\n",
      "Epoch   258:    TrainErr    0.00%    ValidErr    1.95%     Time:  35.92s\n",
      "Epoch   259:    TrainErr    0.00%    ValidErr    1.95%     Time:  35.68s\n",
      "Epoch   260:    TrainErr    0.00%    ValidErr    1.95%     Time:  34.85s\n",
      "Epoch   261:    TrainErr    0.00%    ValidErr    1.95%     Time:  32.79s\n",
      "Epoch   262:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.23s\n",
      "Epoch   263:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.45s\n",
      "Epoch   264:    TrainErr    0.00%    ValidErr    1.95%     Time:  30.09s\n",
      "Epoch   265:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.94s\n",
      "Epoch   266:    TrainErr    0.00%    ValidErr    1.95%     Time:  31.88s\n",
      "Epoch   267:    TrainErr    0.00%    ValidErr    1.94%     Time:  32.94s\n",
      "Epoch   268:    TrainErr    0.00%    ValidErr    1.94%     Time:  32.14s\n",
      "Epoch   269:    TrainErr    0.00%    ValidErr    1.94%     Time:  33.01s\n",
      "Epoch   270:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.69s\n",
      "Epoch   271:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.73s\n",
      "Epoch   272:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.65s\n",
      "Epoch   273:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.30s\n",
      "Epoch   274:    TrainErr    0.00%    ValidErr    1.94%     Time:  31.06s\n",
      "Epoch   275:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.26s\n",
      "Epoch   276:    TrainErr    0.00%    ValidErr    1.94%     Time:  31.27s\n",
      "Epoch   277:    TrainErr    0.00%    ValidErr    1.94%     Time:  31.08s\n",
      "Epoch   278:    TrainErr    0.00%    ValidErr    1.93%     Time:  31.79s\n",
      "Epoch   279:    TrainErr    0.00%    ValidErr    1.93%     Time:  30.96s\n",
      "Epoch   280:    TrainErr    0.00%    ValidErr    1.93%     Time:  30.16s\n",
      "Epoch   281:    TrainErr    0.00%    ValidErr    1.93%     Time:  29.86s\n",
      "Epoch   282:    TrainErr    0.00%    ValidErr    1.93%     Time:  31.32s\n",
      "Epoch   283:    TrainErr    0.00%    ValidErr    1.93%     Time:  29.83s\n",
      "Epoch   284:    TrainErr    0.00%    ValidErr    1.93%     Time:  34.19s\n",
      "Epoch   285:    TrainErr    0.00%    ValidErr    1.93%     Time:  30.18s\n",
      "Epoch   286:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.05s\n",
      "Epoch   287:    TrainErr    0.00%    ValidErr    1.94%     Time:  37.25s\n",
      "Epoch   288:    TrainErr    0.00%    ValidErr    1.94%     Time:  32.80s\n",
      "Epoch   289:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.76s\n",
      "Epoch   290:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.60s\n",
      "Epoch   291:    TrainErr    0.00%    ValidErr    1.94%     Time:  31.35s\n",
      "Epoch   292:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.16s\n",
      "Epoch   293:    TrainErr    0.00%    ValidErr    1.94%     Time:  28.78s\n",
      "Epoch   294:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.53s\n",
      "Epoch   295:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.23s\n",
      "Epoch   296:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.90s\n",
      "Epoch   297:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.34s\n",
      "Epoch   298:    TrainErr    0.00%    ValidErr    1.94%     Time:  29.54s\n",
      "Epoch   299:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.53s\n",
      "Epoch   300:    TrainErr    0.00%    ValidErr    1.94%     Time:  30.12s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-83c8a8ccfb4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearningrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-6ef6fe2ad648>\u001b[0m in \u001b[0;36mbprop\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdh\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdh\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdh\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdW\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdW\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdW\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdb\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdb\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdLdb\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2878\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 2879\u001b[1;33m                           out=out, keepdims=keepdims)\n\u001b[0m\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Import packages for loading\n",
    "import cPickle\n",
    "import gzip\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#Load data\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#Hyperparameters.\n",
    "B   = 100\n",
    "learningrate = 0.04\n",
    "\n",
    "Nt  = train_set[0].shape[0]\n",
    "Nv  = valid_set[0].shape[0]\n",
    "\n",
    "ni  = train_set[0].shape[1]\n",
    "nh  = 200\n",
    "no  = 10\n",
    "\n",
    "#Create MLP object\n",
    "mlp = MLP(ni,nh,no)\n",
    "\n",
    "#Run training loop\n",
    "iteration = 0\n",
    "while True:\n",
    "    tstart = time.time()\n",
    "    \n",
    "    if(iteration > 0):\n",
    "        #Run a full epoch of training\n",
    "        for i in xrange(Nt/B):\n",
    "            x = train_set[0][i*B:i*B+B]\n",
    "            y = train_set[1][i*B:i*B+B]\n",
    "            t = np.eye(no)[y]\n",
    "\n",
    "            mlp.bprop(x,t)\n",
    "            mlp.update(learningrate)\n",
    "\n",
    "            #sys.stdout.write(\"Epoch [{:5d}] TRAINING [{:5d}/{:5d}]\\r\".format(iteration, i*B, Nt))\n",
    "            #sys.stdout.write(\".\")\n",
    "            #sys.stdout.flush()\n",
    "        #sys.stdout.write(\"                                                \\r\")\n",
    "        #sys.stdout.write(\"\\n\")\n",
    "        #sys.stdout.flush()\n",
    "    \n",
    "    #Count training and validation errors\n",
    "    trainerrs = 0\n",
    "    validerrs = 0\n",
    "    \n",
    "    for i in xrange(Nt/B):\n",
    "        x = train_set[0][i*B:i*B+B]\n",
    "        y = train_set[1][i*B:i*B+B]\n",
    "        t = np.eye(no)[y]\n",
    "        \n",
    "        mlp.fprop(x,t)\n",
    "        \n",
    "        yest = np.argmax(mlp.y, axis=-1)\n",
    "        trainerrs += np.sum(yest != y)\n",
    "        \n",
    "        #sys.stdout.write(\"Epoch [{:5d}] COMPUTING TRAIN ERR [{:5d}/{:5d}]\\r\".format(iteration, i*B, Nt))\n",
    "        #sys.stdout.write(\".\")\n",
    "        #sys.stdout.flush()\n",
    "    #sys.stdout.write(\"                                                                                \\r\")\n",
    "    #sys.stdout.write(\"\\n\")\n",
    "    #sys.stdout.flush()\n",
    "    \n",
    "    for i in xrange(Nv/B):\n",
    "        x = valid_set[0][i*B:i*B+B]\n",
    "        y = valid_set[1][i*B:i*B+B]\n",
    "        t = np.eye(no)[y]\n",
    "        \n",
    "        mlp.fprop(x,t)\n",
    "        \n",
    "        yest = np.argmax(mlp.y, axis=-1)\n",
    "        validerrs += np.sum(yest != y)\n",
    "        \n",
    "        #sys.stdout.write(\"Epoch [{:5d}] COMPUTING VALID ERR [{:5d}/{:5d}]\\r\".format(iteration, i*B, Nv))\n",
    "        #sys.stdout.write(\".\")\n",
    "        #sys.stdout.flush()\n",
    "    #sys.stdout.write(\"                                                                                \\r\")\n",
    "    #sys.stdout.write(\"\\n\")\n",
    "    #sys.stdout.flush()\n",
    "    \n",
    "    tend   = time.time()\n",
    "    \n",
    "    #Print training and validation errors\n",
    "    sys.stdout.write(\"Epoch {:5d}:    TrainErr  {:6.3f}%    ValidErr  {:6.2f}%     Time: {:6.2f}s\\n\".format(\n",
    "        iteration,\n",
    "        100.0 * trainerrs / Nt,\n",
    "        100.0 * validerrs / Nv,\n",
    "        tend-tstart\n",
    "        ))\n",
    "    sys.stdout.flush()\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
